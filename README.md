<h1>About the Project</h1>
Capstone project at PES University.

This project aims to simplify the choreography process for dancers and choreographers by creating an automated system that maps dance moves to the beats of a music piece. 
The system uses audio analysis, 3D animation, and contextual factors like background, setting, costume, and lighting to create a visually pleasing and immersive experience that accurately represents the musical characteristics of the input file.
 
The system analyzes the tempo, rhythm, tone, and emotion of a music file to generate synchronized dance moves that match the music's style and mood. 
It creates a realistic 3D dancing figure to simulate fluid, life-like movements. Additionally, it provides recommendations for enhancing performances by analyzing contextual factors such as lighting, stage design, and costumes. 
By automating the choreography process, the system offers a streamlined, user-friendly solution for creating immersive, visually and audibly cohesive dance performances.

Below is an image of the UI which integrates the **dance, costume and background settings** for a particular song, created using Streamlit.
![image](https://github.com/user-attachments/assets/5d248aa7-107c-44c0-9e36-3ded6677c074)

Additionally, we developed a **Bharatnatyam dance dataset** using OpenPose, MediaPipe, and SMPL points, which can serve as a valuable resource for future research in classical dance choreography.
3D reconstruction with SMPL points:
![image](https://github.com/user-attachments/assets/e68849ea-f429-415b-aca6-f7d3769aa4e3)
